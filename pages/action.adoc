= PrÃ©sentation du terrain d'action
:imagesdir: assets/default/images
image::mi-action.png[]
//mi-4
[NOTE.speaker]
====
Maintenant que notre environnement d'entraÃ®nement est en place, Il est temps maintenant de **passer Ã  lâ€™action**, sur le terrain rÃ©el.
====

== Choix stratÃ©gique : AWS EKS

[NOTE.speaker]
====
ğŸ§­ Premier choix dÃ©cisif : **le terrain dâ€™opÃ©ration**.  
Nous avons besoin dâ€™une plateforme capable dâ€™encaisser 511 clusters sans flancher.

ğŸ¯ Cible retenue : **AWS EKS**

* âœ… maÃ®trise d'AWS et d'EKS
* ğŸ›¡ï¸ Confiance pour supporter 511 clusters
====

== Limites imposÃ©es par AWS

image::service-quota.png[]

[NOTE.speaker]
====
ğŸ’¥ Premier obstacle : AWS bloque Ã  **100 clusters max**.

ğŸ© Tentative : nÃ©gociation.

â³ AprÃ¨s plusieurs semaines : refus pour 511.

âœ… RÃ©sultat obtenu : **256 clusters** autorisÃ©s.

ğŸ”¥ OK, AWS ne nous laisse pas faire 511 clusters.

Mais honnÃªtement, aprÃ¨s nos premiers tests qui bloquaient Ã  **15 clusters**,  
ğŸ¯ **256**, câ€™est dÃ©jÃ  un sacrÃ© objectif.

Et surtout, on nâ€™est pas au bout de nos surprisesâ€¦

Parlons maintenant des **bonnes pratiques AWS**â€¦ et de la rÃ©alitÃ© terrain.
====

== Bonnes pratiques vs RÃ©alitÃ© opÃ©rationnelle

[NOTE.speaker]
====
ğŸ§  Je connais bien AWS. MÃªme une certif pour le prouver.

ğŸ“˜ Selon les bonnes pratiques AWS, il faut :
  * crÃ©er des Node Pools,
  * qui crÃ©ent des ASG,
  * qui crÃ©ent des EC2.

ğŸš¨ Sauf que tout Ã§a prend un **temps fou**.

Et dans une mission oÃ¹ chaque seconde compte,
â© On coupe dans le gras.

ğŸ¯ On va donc crÃ©er **directement des EC2**, sans Node Pool, sans ASG.

Câ€™est un anti-pattern ? Absolument.
Mais câ€™est le seul moyen de rester dans la fenÃªtre de tir.

ğŸ‘‰ Allez, regardons maintenant lâ€™architecture retenue pour aller jusquâ€™Ã  256 clusters.
====

== Architecture retenue

image::aws-archi.svg[width=50%]

[NOTE.speaker]
====
Voici l'architecture que j'ai mis au point pour essayer de dÃ©ployer jusqu'Ã  256 clusters Kubernetes
On a un compte AWS, un VPC, un rÃ©seau privÃ©. Ã€ l'intÃ©rieur il contient 4 sous-rÃ©seaux 2 publics 2 privÃ©s
Dans les sous-rÃ©seaux public il y a une NAT Gateway pour pouvoir tÃ©lÃ©charger les images des containers
Dans les rÃ©seaux privÃ©, il y a les clusters EKS avec une seule EC2 et un control plane
====

== ParallÃ©lisation des connexions

image::connection-answer.apng[width=45%]
[NOTE.speaker]
====
Une des principales difficultÃ©s de la mission est la crÃ©ation des connexions. Je vous prÃ©sente la premiÃ¨re tentative pour parallÃ©liser les connexions.

La contrainte : pas de crÃ©ation des connexions d'un mÃªme cluster en parallÃ¨le

Ainsi avec cet algorithme, avec 6 clusters kubernetes on a 5 Ã©tapes.

Avec cet algorithme, on passe d'une complexitÃ© de O(n2) Ã  O(n).
====

== 16 clusters

image::16-clusters.apng[width=50%]

[NOTE.speaker]
====
Le test de 32 clusters a Ã©chouÃ©, j'ai rÃ©duit la voilure Ã  16 clusters

âŒ Mur technique dÃ©tectÃ© :

* ğŸ“¦ Trop dâ€™objets Pulumi â†’ explosion de la RAM ğŸ’¥
* ğŸ” Connexions entre clusters â†’ explosion du CPU
  * 1 connexion â‰ˆ 1 CPU utilisÃ©
  * 128 connexions = 128 CPUs ? ğŸ˜…

ğŸ“‰ RÃ©sultat :

* âœ… 16 clusters connectÃ©s
* â±ï¸ 45 minutesâ€¦
* ğŸš« Bien trop long pour 511 clusters

ğŸ’¡ Conclusion :
    Il faut une autre stratÃ©gie de connexion.
====
