= Présentation du terrain d'action
:imagesdir: assets/default/images
image::mi-action.png[]
//mi-4
[NOTE.speaker]
====
Maintenant que notre environnement d'entraînement est en place, Il est temps maintenant de **passer à l’action**, sur le terrain réel.
====

== Choix stratégique : AWS EKS

[NOTE.speaker]
====
🧭 Premier choix décisif : **le terrain d’opération**.  
Nous avons besoin d’une plateforme capable d’encaisser 511 clusters sans flancher.

🎯 Cible retenue : **AWS EKS**

* ✅ maîtrise d'AWS et d'EKS
* 🛡️ Confiance pour supporter 511 clusters
====

== Limites imposées par AWS

image::service-quota.png[]

[NOTE.speaker]
====
💥 Premier obstacle : AWS bloque à **100 clusters max**.

🎩 Tentative : négociation.

⏳ Après plusieurs semaines : refus pour 511.

✅ Résultat obtenu : **256 clusters** autorisés.

🔥 OK, AWS ne nous laisse pas faire 511 clusters.

Mais honnêtement, après nos premiers tests qui bloquaient à **15 clusters**,  
🎯 **256**, c’est déjà un sacré objectif.

Et surtout, on n’est pas au bout de nos surprises…

Parlons maintenant des **bonnes pratiques AWS**… et de la réalité terrain.
====

== Bonnes pratiques vs Réalité opérationnelle

[NOTE.speaker]
====
🧠 Je connais bien AWS. Même une certif pour le prouver.

📘 Selon les bonnes pratiques AWS, il faut :
  * créer des Node Pools,
  * qui créent des ASG,
  * qui créent des EC2.

🚨 Sauf que tout ça prend un **temps fou**.

Et dans une mission où chaque seconde compte,
⏩ On coupe dans le gras.

🎯 On va donc créer **directement des EC2**, sans Node Pool, sans ASG.

C’est un anti-pattern ? Absolument.
Mais c’est le seul moyen de rester dans la fenêtre de tir.

👉 Allez, regardons maintenant l’architecture retenue pour aller jusqu’à 256 clusters.
====

== Architecture retenue

image::aws-archi.svg[width=50%]

[NOTE.speaker]
====
Voici l'architecture que j'ai mis au point pour essayer de déployer jusqu'à 256 clusters Kubernetes
On a un compte AWS, un VPC, un réseau privé. À l'intérieur il contient 4 sous-réseaux 2 publics 2 privés
Dans les sous-réseaux public il y a une NAT Gateway pour pouvoir télécharger les images des containers
Dans les réseaux privé, il y a les clusters EKS avec une seule EC2 et un control plane
====

== Parallélisation des connexions

image::connection-answer.apng[width=45%]
[NOTE.speaker]
====
Une des principales difficultés de la mission est la création des connexions. Je vous présente la première tentative pour paralléliser les connexions.

La contrainte : pas de création des connexions d'un même cluster en parallèle

Ainsi avec cet algorithme, avec 6 clusters kubernetes on a 5 étapes.

Avec cet algorithme, on passe d'une complexité de O(n2) à O(n).
====

== 16 clusters

image::16-clusters.apng[width=50%]

[NOTE.speaker]
====
Le test de 32 clusters a échoué, j'ai réduit la voilure à 16 clusters

❌ Mur technique détecté :

* 📦 Trop d’objets Pulumi → explosion de la RAM 💥
* 🔁 Connexions entre clusters → explosion du CPU
  * 1 connexion ≈ 1 CPU utilisé
  * 128 connexions = 128 CPUs ? 😅

📉 Résultat :

* ✅ 16 clusters connectés
* ⏱️ 45 minutes…
* 🚫 Bien trop long pour 511 clusters

💡 Conclusion :
    Il faut une autre stratégie de connexion.
====
