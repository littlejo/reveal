= ğŸ§—EntraÃ®nement de l'agent
:imagesdir: assets/default/images
image::mi-training.png[]
//mi-2
[NOTE.speaker]
====
Maintenant que la mission est bien dÃ©finie, il est temps de passer Ã  lâ€™entraÃ®nement.

Pour Ã§a, on va se mettre en condition rÃ©elleâ€¦ mais Ã  petite Ã©chelle.
====

== Terrain dâ€™exercice : KinD

* Un seul serveur
* Docker

image::kind-logo.png[width=45%]

[NOTE.speaker]
====
On va utiliser un terrain dâ€™exercice simple, efficace, et rapide Ã  dÃ©ployer : KinD, Kubernetes IN Docker.

Distribution Kubernetes
====

== CI/CD : GitHub Action

image::github-action.png[]

[NOTE.speaker]
====
On a choisi KinD pour son cÃ´tÃ© lÃ©ger et rapide, idÃ©al pour simuler nos clusters sur un seul serveur.

Mais rÃ©pÃ©ter ces tests Ã  la main serait fastidieux.

Câ€™est pourquoi on automatise tout avec une pipeline CI/CD.

* "Gratuit"
* VM temporaires avec 4 CPU pour simuler des missions
====

== Infrastructure as code

image::iac.apng[]

[NOTE.speaker]
====
GitHub Actions, câ€™est parfait pour rejouer nos tests automatiquement Ã  chaque changement.

Mais il nous faut aussi un outil pour gÃ©rer **lâ€™infrastructure elle-mÃªme** :
CrÃ©er, connecter et dÃ©truire 511 clusters â€” sans intervention manuelle.

Câ€™est lÃ  quâ€™intervient lâ€™**Infrastructure as Code**.

ğŸ§° Options du QG :

* Terraform / Opentofu
* Pulumi
* Crossplane

====

== Pulumi

[source,python,linenums]
----
kind = local.Command("kindCluster",
    create="kind create cluster --config kind.yaml --name cmesh1"
)

kind2 = local.Command("kindCluster2",
    create="kind create cluster --config kind-2.yaml --name cmesh2"
)

cmesh1_provider = cilium.Provider("cmesh1", context="kind-cmesh1", opts=pulumi.ResourceOptions(depends_on=[kind]))
cmesh2_provider = cilium.Provider("cmesh2", context="kind-cmesh2", opts=pulumi.ResourceOptions(depends_on=[kind2]))

cmesh1_cilium = cilium.Install("cmesh1Install",
    sets=[
        "cluster.name=cmesh1",
        "cluster.id=1",
        "ipam.mode=kubernetes",
    ],
    version="1.15.5",
    opts=pulumi.ResourceOptions(depends_on=[kind], providers=[cmesh1_provider]),
)

cmesh2_cilium = cilium.Install("cmesh2Install",
    sets=[
        "cluster.name=cmesh2",
        "cluster.id=2",
        "ipam.mode=kubernetes",
    ],
    version="1.15.5",
    opts=pulumi.ResourceOptions(depends_on=[kind2], providers=[cmesh2_provider]),
)

cmesh1_cmeshenable = cilium.Clustermesh("cmesh1Enable", service_type="NodePort", opts=pulumi.ResourceOptions(depends_on=[cmesh1_cilium], providers=[cmesh1_provider]))
cmesh2_cmeshenable = cilium.Clustermesh("cmesh2Enable", service_type="NodePort", opts=pulumi.ResourceOptions(depends_on=[cmesh2_cilium], providers=[cmesh2_provider]))

cilium.ClustermeshConnection("cmeshConnect", destination_context="kind-cmesh2", opts=pulumi.ResourceOptions(depends_on=[cmesh1_cmeshenable], providers=[cmesh1_provider]))
----


[NOTE.speaker]
====
ğŸ•¶ï¸ DÃ©cision : Pulumi

Pourquoi ?

* Souplesse car c'est du code orientÃ© dÃ©veloppement
* Apprendre autre chose que terraform : comparatif

Autre choix :

* langage de programmation

ğŸ•¶ï¸ DÃ©cision : Python
====

== Tester les limites de KinD
image::15-clusters.apng[width=50%]

[NOTE.speaker]
====
Souvent, Ã  la fin dâ€™un entraÃ®nement, on cherche Ã  **pousser les limites** des outils utilisÃ©s.

ğŸ›ï¸ MatÃ©riel utilisÃ© :

* ğŸ–¥ï¸ 16 CPU â€” ğŸ§  32 Go de RAM

ğŸš« RÃ©sultat :

* Blocage Ã  15 clusters maximum
* Temps de dÃ©ploiement : 45 minutes

ğŸ’£ Bien trop long pour 511 clusters.
====
