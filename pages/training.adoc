= 🧗Entraînement de l'agent
:imagesdir: assets/default/images
image::mi-training.png[]
//mi-2
[NOTE.speaker]
====
Maintenant que la mission est bien définie, il est temps de passer à l’entraînement.

Pour ça, on va se mettre en condition réelle… mais à petite échelle.
====

== Terrain d’exercice : KinD

* Un seul serveur
* Docker

image::kind-logo.png[width=45%]

[NOTE.speaker]
====
On va utiliser un terrain d’exercice simple, efficace, et rapide à déployer : KinD, Kubernetes IN Docker.

Distribution Kubernetes
====

== CI/CD : GitHub Action

image::github-action.png[]

[NOTE.speaker]
====
On a choisi KinD pour son côté léger et rapide, idéal pour simuler nos clusters sur un seul serveur.

Mais répéter ces tests à la main serait fastidieux.

C’est pourquoi on automatise tout avec une pipeline CI/CD.

* "Gratuit"
* VM temporaires avec 4 CPU pour simuler des missions
====

== Infrastructure as code

image::iac.apng[]

[NOTE.speaker]
====
GitHub Actions, c’est parfait pour rejouer nos tests automatiquement à chaque changement.

Mais il nous faut aussi un outil pour gérer **l’infrastructure elle-même** :
Créer, connecter et détruire 511 clusters — sans intervention manuelle.

C’est là qu’intervient l’**Infrastructure as Code**.

🧰 Options du QG :

* Terraform / Opentofu
* Pulumi
* Crossplane

====

== Pulumi

[source,python,linenums]
----
kind = local.Command("kindCluster",
    create="kind create cluster --config kind.yaml --name cmesh1"
)

kind2 = local.Command("kindCluster2",
    create="kind create cluster --config kind-2.yaml --name cmesh2"
)

cmesh1_provider = cilium.Provider("cmesh1", context="kind-cmesh1", opts=pulumi.ResourceOptions(depends_on=[kind]))
cmesh2_provider = cilium.Provider("cmesh2", context="kind-cmesh2", opts=pulumi.ResourceOptions(depends_on=[kind2]))

cmesh1_cilium = cilium.Install("cmesh1Install",
    sets=[
        "cluster.name=cmesh1",
        "cluster.id=1",
        "ipam.mode=kubernetes",
    ],
    version="1.15.5",
    opts=pulumi.ResourceOptions(depends_on=[kind], providers=[cmesh1_provider]),
)

cmesh2_cilium = cilium.Install("cmesh2Install",
    sets=[
        "cluster.name=cmesh2",
        "cluster.id=2",
        "ipam.mode=kubernetes",
    ],
    version="1.15.5",
    opts=pulumi.ResourceOptions(depends_on=[kind2], providers=[cmesh2_provider]),
)

cmesh1_cmeshenable = cilium.Clustermesh("cmesh1Enable", service_type="NodePort", opts=pulumi.ResourceOptions(depends_on=[cmesh1_cilium], providers=[cmesh1_provider]))
cmesh2_cmeshenable = cilium.Clustermesh("cmesh2Enable", service_type="NodePort", opts=pulumi.ResourceOptions(depends_on=[cmesh2_cilium], providers=[cmesh2_provider]))

cilium.ClustermeshConnection("cmeshConnect", destination_context="kind-cmesh2", opts=pulumi.ResourceOptions(depends_on=[cmesh1_cmeshenable], providers=[cmesh1_provider]))
----


[NOTE.speaker]
====
🕶️ Décision : Pulumi

Pourquoi ?

* Souplesse car c'est du code orienté développement
* Apprendre autre chose que terraform : comparatif

Autre choix :

* langage de programmation

🕶️ Décision : Python
====

== Tester les limites de KinD
image::15-clusters.apng[width=50%]

[NOTE.speaker]
====
Souvent, à la fin d’un entraînement, on cherche à **pousser les limites** des outils utilisés.

🎛️ Matériel utilisé :

* 🖥️ 16 CPU — 🧠 32 Go de RAM

🚫 Résultat :

* Blocage à 15 clusters maximum
* Temps de déploiement : 45 minutes

💣 Bien trop long pour 511 clusters.
====
